
# Zillow project, Linear Regression

```{r}
rm(list=ls())
```

# Import data
```{r}
train<-read.csv('train_rename.csv',stringsAsFactors = F)
```
train_rename is the renamed train_property dataset.


# Prepare data

## Correct categorical data type
```{r}
# categorical variable
variable_nominal = c("aircon",
                     "architectural_style",
                     "county",
                     "deck",
                     "framing",
                     "heating",
                     "id_parcel",
                     "material",
                     "region_city",
                     "region_county",
                     "region_neighbor",
                     "region_zip",
                     "story",
                     "zoning_landuse",
                     "zoning_landuse_county"
                     )
variable_ordinal = c("quality")

# convert to categorical to character
train[,c(variable_nominal, variable_ordinal)] = sapply(train[,c(variable_nominal, variable_ordinal)], as.character)
```


## Deal with flag features
```{r}
#fill flag NA with 0
train$flag_tub= as.numeric(train$flag_tub)
train[which(is.na(train$flag_tub)), "flag_tub"] = 0
train[which(is.na(train$flag_spa)), "flag_spa"] = 0
train[which(is.na(train$flag_pool_spa)), "flag_pool_spa"] = 0
train[which(is.na(train$flag_pool_tub)), "flag_pool_tub"] = 0
train$flag_fireplace = as.numeric(train$flag_fireplace)
train[which(is.na(train$flag_fireplace)), "flag_fireplace"] = 0
```

## generate num_room bug feature
Create a binary room bug feature. If bathroom number + bedroom number < room number, it means there are errors in data. I give 1 value to these error datas.
```{r}
train$room_bug <- ifelse(train$num_bathroom + train$num_bedroom < train$num_room, 1,0)
plot(density(train$room_bug))
```


## Heating features
```{r}
train[which(is.na(train$aircon)), "aircon"] = "0"
train[which(is.na(train$heating)), "heating"] = "0"
```

## Redundant features
## num_bathroom, num_bathroom_calc
```{r}
library(ggplot2)
df = train[!is.na(train$num_bathroom_calc),
           c("num_bathroom","num_bathroom_calc")]
str(df)
ggplot(data = df,
       aes(x=num_bathroom,
           y=num_bathroom_calc)) + geom_point()
sum(df$num_bathroom == df$num_bathroom_calc) == nrow(df)
```
These two columns are the same. Drop num_bathroom_calc.

## num_bathroom, num_bath
```{r}
df = train[!is.na(train$num_bath),
           c("num_bathroom","num_bath")]
ggplot(data = df,
       aes(x=num_bath,
           y=num_bathroom)) + geom_point()
sum(df$num_bath == df$num_bathroom)/nrow(df)
```
These two columns are nearly the same. Drop num_bath.

## new features generated from HW2
## age of the house
```{r}
train$age<-(2017-train$build_year+1) 
```

## tax ratio, tax_bug
tax ratio = tax_property / tax_total
```{r}
#tax ratio
train$tax_ratio <- (train$tax_property / train$tax_total)
nrow(subset(train,tax_ratio > 1))

#tax_bug
train$tax_bug <- ifelse(train$tax_ratio > 1, 1,0)
table(train$tax_bug)

```
There are only 5 obvious tax errors, so we don't use the tax_bug feature.

## tax of built structure/ total tax
```{r}
train$tax_ratiobuilt <- (train$tax_building / train$tax_total)
```

This may not be a good feature since the correlation is still low.

## Area
## lot_living, lot_bug
lot_living = area_lot / area_live_finished
```{r}
#lot_living
train$lot_living<- (train$area_lot / train$area_live_finished)
summary(train$lot_living)
nrow(subset(train, lot_living < 1))

#log_bug
train$lot_bug <- ifelse(train$lot_living < 1 , 1,0)
table(train$lot_bug)
```

Since lot area should always be bigger than the living area, the lot_living with number smaller than one must be errors. We generate a new feature lot_bug.

## Room number
## bedroom/bathroom ratio
```{r}
train$bbratio<-(train$num_bedroom/(train$num_bathroom+1))
```

***save my preparation of data***
```{r}
#write.csv(train, "train.prep.csv")
#train <- read.csv("train.prep.csv")
```


#Features to use:
## Original features
num_bathroom
num_bedroom
tax_building
area_total_calc
area_live_finished
tax_property
area_lot
build_year
county
zoning_landuse_county
aircon
heating
flag_tub
flag_spa
flag_pool_spa
flag_pool_tub
flag_fireplace
region_zip
region_county
zoning_landuse

## New features
age of hourse: age
tax ratio: tax_ratio
tax of built structure/ total tax: tax_ratiobuilt
lot area/living area: lot_living
bedroom/bathroom ratio: bbratio
room_bug
lot_bug


```{r}
train.use <- subset(train[,c('logerror','num_bathroom','num_bedroom','tax_building','area_total_calc','area_live_finished','tax_property','area_lot','build_year','county','zoning_landuse_county','aircon','heating','flag_tub','flag_spa','flag_pool_spa','flag_pool_tub','flag_fireplace','age','tax_ratio','tax_ratiobuilt','lot_living','bbratio','region_zip','region_city','region_county','zoning_landuse','room_bug','lot_bug', 'longitude','latitude')])

str(train.use)

```

## Impute missing values
## Check missing values
```{r}
library(Amelia)
missmap(train.use)
missingcol <- colSums(is.na(train.use))
missingcol
```


##  drop uni-value column
```{r}
multi_value = sapply(train.use,
                       function(x){
                         return(!length(unique(x[!is.na(x)])) == 1)})
  if(length(multi_value)!=0)
    train.use = train.use[, names(which(multi_value))]
```

#Train the model: Linear Regression Model
## Standardize
```{r}
train.use$build_year <- as.character(train.use$build_year)
num.col = names(which(sapply(train.use, is.numeric)))
use.num = train.use[,num.col]
train.use[,num.col]=scale(train.use[,num.col])  
```


## Basic linear regression model
```{r}
mod1 <- lm(logerror ~., data = train.use)
plot(mod1)
summary(mod1)
```

## glmnet function
```{r}
library(glmnet)
train.sub <- train.use[which(apply(train.use, 1, 
                                   function(x) length(which(is.na(x))) == 0)), ]  
dim(train.sub)
dep = train.sub$logerror
ind <- model.matrix( ~.-1, train.sub[, -1]) 

```


##  lasso
```{r}
fit_lasso <- glmnet(x=ind, y=dep, alpha = 1)
plot(fit_lasso, xvar = "lambda", label = T)
```

##  use cross validation to get optimal value of lambda
```{r}
cvfit1 <- cv.glmnet(ind, dep, alpha = 1)
plot(cvfit1)

sprintf ("lambda.min: %f ",cvfit1$lambda.min)
x1 = coef(cvfit1, s = "lambda.min")
x1
```


##  calculate r-square
```{r}
y_pred1 = predict(cvfit1,newx=ind,type="response",s="lambda.min")
sst1 <- sum((train.sub$logerror - mean(train.sub$logerror))^2)
sse1 <- sum((y_pred1 - train.sub$logerror)^2)

r.square1 <- 1 - sse1 / sst1
sprintf ("r_square: %f ",r.square1)
```

##  ridge
```{r}
fit_ridge <- glmnet(x=ind, y=dep, alpha = 0)
plot(fit_ridge, xvar ="lambda", label = T)
```

## use cross validation to get optimal value of lambda
```{r}
cvfit2 <- cv.glmnet(ind, dep, alpha = 0)
plot(cvfit2)

sprintf ("lambda.min: %f ",cvfit2$lambda.min)
x2 = coef(cvfit2, s = "lambda.min")
x2
```


## calculate r-square
```{r}
y_pred2 = predict(cvfit2,newx=ind,type="response",s="lambda.min")
sst2 <- sum((train.sub$logerror - mean(train.sub$logerror))^2)
sse2 <- sum((y_pred2 - train.sub$logerror)^2)

r.square2 <- 1 - sse2 / sst2
sprintf ("r_square: %f ",r.square2)
```





