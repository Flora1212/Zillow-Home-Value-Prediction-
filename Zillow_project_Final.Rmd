# Zillow project

# Step one - Collecting data 
This dataset is collected from kaggle zillow challenge.

# Step two - Exploring and preparing the data
# Import data in Rstudio
```{r}
train<-read.csv('train_2016_v2.csv', stringsAsFactors = FALSE)
property <- read.csv('properties_2016.csv', stringsAsFactors = FALSE)
```


# Combine train and property dataset
```{r}
length(unique(train$parcelid))
length(unique(property$parcelid))
property <- subset(property, parcelid %in% train$parcelid)
# select all the rows in property that are accordingly to rows in train$parcelid
train <- merge(train, property, by = 'parcelid', all.x = T)
#all.x = T means left join
```

# Exploring data
## Check the data types
```{r}
library(ggplot2)
types <- sapply(train, class)
df.types <- data.frame(table(types),
                       row.names = NULL) # convert to data.frame
ggplot(data=df.types, aes(x=types, y=Freq/sum(Freq))) + geom_bar(stat = "identity", width=0.2)
```

## Check the missing values
```{r}
num.NA <- sort(colSums(sapply(train, is.na)))
num.NA
# visualize
library(ggplot2)
dfnum.NA <- data.frame(ind = c(1:length(num.NA)),
                       percentage = num.NA/nrow(train),
                       per80 = num.NA/nrow(train)>=0.2,
                       name = names(num.NA),
                       row.names = NULL) # convert to data.frame
ggplot(data = dfnum.NA, aes(x=ind, y=percentage)) + 
  geom_bar(aes(fill=per80), stat="identity") + 
  scale_x_discrete(name ="column names", 
                   limits=dfnum.NA$name)+
  theme(axis.text.x = element_text(angle=90, hjust=1, vjust=.5),
        legend.position = "none") +
  geom_hline(yintercept = 0.2) + 
  annotate("text", 5, 0.21, label="20 percent", size = 5) + 
  annotate("text", 4, 0.35, label="drop", color="#00BFC4",size=5) + 
  annotate("text", 4, 0.05, label="keep", color="#F8766D",size=5) + 
  ggtitle("percentage of missing")
```

## Check the response variable logerror
```{r}
summary(train$logerror)
plot(density(train$logerror))
```
We can see that logerror is very centered to 0, indicating Zestimate works quite well.

## Correlation - numeric variables
To select the variables, we first calculate the correlation between numeric variables and response variable logerror.
```{r}
length(unique(train$propertycountylandusecode))
length(unique(train$propertylandusetypeid))
length(unique(train$propertyzoningdesc))
typeof(train$propertyzoningdesc)
length(unique(train$assessmentyear))
summary(unique(train$taxdelinquencyflag))
typeof(unique(train$latitude))
length(unique(train$taxvaluedollarcnt))


library(corrplot)
correlations <- cor(train[, c('logerror', 'bathroomcnt', 'bedroomcnt', 'latitude','longitude','roomcnt',
                              'taxvaluedollarcnt','landtaxvaluedollarcnt', 
                              'taxamount', 'structuretaxvaluedollarcnt','calculatedfinishedsquarefeet',
                              'calculatedbathnbr', 'fullbathcnt', 'finishedsquarefeet12',
                              'lotsizesquarefeet')],use = "pairwise.complete.obs")
correlations
corrplot(correlations, method = "square", tl.cex = 1, type = 'upper')
```

Here, use = "pairwise.complete.obs" applies where trying to build a correlation matrix (i.e. correlations between more than two variables) and instead of dropping all cases with any missing data, it only drops cases from each pairwise correlation calculation.

From the correlation results, we can see that the correlation between independent variables and response variable logerror are generally low. But we can still choose some with relatively higher correlation: **bathroomcnt,bedroomcnt,structuretaxvaluedollarcnt,calculatedfinishedsquarefeet,calculatedbathnbr,fullbathcnt,
finishedsquarefeet12.** finishedsquarefeet12 has the strongest positive correlation with logerror, while taxamount has the strongest negative correlation with logerror.

Then, we use tabplot to further check each variable.

# Visualization variables
## Tabplot - numeric
## date variables: year-built
```{r}
barplot(table(train$yearbuilt))
err.month_yearbuilt <- by(train, train$yearbuilt, function(x) {
  return(mean(x$logerror))})
plot(names(err.month_yearbuilt), err.month_yearbuilt, type = 'l', xlab='year_built',ylab='mean_logerror')
```
The graph shows that the predictions for the early-year built houses are less accurate with higher mean_logerror ,especially the houses built before 1900. After 1940, the predictions become much better. However, recently underestimates are increasing.

## number variables
```{r}
library(tabplot)
tableplot(train, select = c('logerror', 'bathroomcnt', 'bedroomcnt', 'roomcnt',
                              'calculatedbathnbr', 'fullbathcnt'))
plot(density(train$roomcnt))

```
From the graph we can see that when absolute logerror is higher,roomcnt tend to be lower. 

## area variables
```{r}
tableplot(train, select = c('logerror', 
                             'calculatedfinishedsquarefeet',
                             'finishedsquarefeet12','lotsizesquarefeet'))
```
We can see that underestimate zestimate tends to go with higher calculatedfinishedsquarefeet and finishedsquarefeet12, overestimate zestimate tends to go with lower calculatedfinishedsquarefeet and finishedsquarefeet12, while lotsizesquarefeet doesn't have obvious pattern.
 
## location variables
```{r}
library(GGally)
sub_train = train[, c('logerror', 'longitude', 'latitude')]
sub_train.dropna = sub_train[complete.cases(sub_train),]
ggpairs(sub_train.dropna, method = "square", tl.cex = 1, type = 'lower')
```
We can see obviously that intemediate latitude gathers more non-zero logerror, and intermediate towards west longitude gather more non-zero logerror.

## value variables
```{r}
tableplot(train, select = c('logerror', 'landtaxvaluedollarcnt', 
                              'taxamount', 'structuretaxvaluedollarcnt'))
```

## Tabplot - categorical
## location variables
6037 Los Angeles, 6059 Orange County, 6111 Ventura County
```{r}
train$fips <- as.character(train$fips)
train$regionidzip <- as.character(train$regionidzip)
train$regionidcity <- as.character(train$regionidcity)
table(train$regionidcounty)
tableplot(train, select = c('logerror','fips','propertycountylandusecode','regionidzip'))
```
The graph indicates that prediction of houses in Los Angeles are less accurate.

## other variables
```{r}
prop.table(table(train$hashottuborspa))
prop.table(table(train$fireplaceflag))
table(train$propertylandusetypeid)

tableplot(train, select = c('logerror','propertyzoningdesc'))

```

# Preparing data
## Rename the features in train dataset
```{r}
train_r <- plyr::rename(train,
                     c("parcelid"="id_parcel",
                       "transactiondate" = "trans_date",
                       "yearbuilt" = "build_year",
                       "basementsqft"="area_base_living",
                       "yardbuildingsqft17"="area_patio",
                       "yardbuildingsqft26"="area_shed",
                       "poolsizesum"="area_pool",
                       "lotsizesquarefeet"="area_lot",
                       "garagetotalsqft"="area_garage",
                       "finishedfloor1squarefeet" = "area_firstfloor_finished",
                       "calculatedfinishedsquarefeet" = "area_total_calc",
                       "finishedsquarefeet6" = "area_base",
                       "finishedsquarefeet12" = "area_live_finished",
                       "finishedsquarefeet13" = "area_liveperi_finished",
                       "finishedsquarefeet15" = "area_total_finished",
                       "finishedsquarefeet50" = "area_unknown",
                       "unitcnt" = "num_unit",
                       "numberofstories" = "num_story",
                       "roomcnt" = "num_room",
                       "bathroomcnt" = "num_bathroom",
                       "bedroomcnt" = "num_bedroom",
                       "calculatedbathnbr" = "num_bathroom_calc",
                       "fullbathcnt" = "num_bath",
                       "threequarterbathnbr" = "num_75_bath",
                       "fireplacecnt" = "num_fireplace",
                       "poolcnt" = "num_pool",
                       "garagecarcnt" = "num_garage",
                       "regionidcounty" = "region_county",
                       "regionidcity" = "region_city",
                       "regionidzip" = "region_zip",
                       "regionidneighborhood" = "region_neighbor",
                       "taxvaluedollarcnt" = "tax_total",
                       "structuretaxvaluedollarcnt" = "tax_building",
                       "landtaxvaluedollarcnt" = "tax_land",
                       "taxamount" = "tax_property",
                       "assessmentyear" = "tax_year",
                       "taxdelinquencyflag" = "tax_delinquency",
                       "taxdelinquencyyear" = "tax_delinquency_year",
                       "propertyzoningdesc" = "zoning_property",
                       "propertylandusetypeid" = "zoning_landuse",
                       "propertycountylandusecode" = "zoning_landuse_county",
                       "fireplaceflag" = "flag_fireplace",
                       "hashottuborspa" = "flag_tub",
                       "buildingqualitytypeid" = "quality",
                       "buildingclasstypeid" = "framing",
                       "typeconstructiontypeid" = "material",
                       "decktypeid" = "deck",
                       "storytypeid" = "story",
                       "heatingorsystemtypeid" = "heating",
                       "airconditioningtypeid" = "aircon",
                       "architecturalstyletypeid" = "architectural_style",
                       "pooltypeid10" = "flag_spa",
                       "pooltypeid2" = "flag_pool_spa",
                       "pooltypeid7" = "flag_pool_tub",
                       "fips"="county"))
write.csv(train_r, 'train_rename.csv')
```

```{r}
train = read.csv("train_rename.csv", stringsAsFactors = F)
```

## Correct categorical data type
```{r}
# categorical variable
variable_nominal = c("aircon",
                     "architectural_style",
                     "county",
                     "deck",
                     "framing",
                     "heating",
                     "id_parcel",
                     "material",
                     "region_city",
                     "region_county",
                     "region_neighbor",
                     "region_zip",
                     "story",
                     "zoning_landuse",
                     "zoning_landuse_county"
                     )
variable_ordinal = c("quality")

# convert to categorical to character
train[,c(variable_nominal, variable_ordinal)] = sapply(train[,c(variable_nominal, variable_ordinal)], as.character)
```

## Deal with flag features
```{r}
#fill flag NA with 0
train$flag_tub= as.numeric(train$flag_tub)
train[which(is.na(train$flag_tub)), "flag_tub"] = 0
train[which(is.na(train$flag_spa)), "flag_spa"] = 0
train[which(is.na(train$flag_pool_spa)), "flag_pool_spa"] = 0
train[which(is.na(train$flag_pool_tub)), "flag_pool_tub"] = 0
train$flag_fireplace = as.numeric(train$flag_fireplace)
train[which(is.na(train$flag_fireplace)), "flag_fireplace"] = 0
```

## Feature Engineering
### num_room bug feature
Create a binary room bug feature. If bathroom number + bedroom number < room number, it means there are errors in data. I give 1 value to these error datas.
```{r}
train$room_bug <- ifelse(train$num_bathroom + train$num_bedroom < train$num_room, 1,0)
plot(density(train$room_bug))
```


### Heating feature
```{r}
train[which(is.na(train$aircon)), "aircon"] = "0"
train[which(is.na(train$heating)), "heating"] = "0"
```

### Redundant features
### num_bathroom, num_bathroom_calc
```{r}
library(ggplot2)
df = train[!is.na(train$num_bathroom_calc),
           c("num_bathroom","num_bathroom_calc")]
str(df)
ggplot(data = df,
       aes(x=num_bathroom,
           y=num_bathroom_calc)) + geom_point()
sum(df$num_bathroom == df$num_bathroom_calc) == nrow(df)
```
These two columns are the same. Drop num_bathroom_calc.

### num_bathroom, num_bath
```{r}
df = train[!is.na(train$num_bath),
           c("num_bathroom","num_bath")]
ggplot(data = df,
       aes(x=num_bath,
           y=num_bathroom)) + geom_point()
sum(df$num_bath == df$num_bathroom)/nrow(df)
```
These two columns are nearly the same. Drop num_bath.

### age of the house
```{r}
train$age<-(2017-train$build_year+1) 
```

### tax ratio, tax_bug
tax ratio = tax_property / tax_total
```{r}
#tax ratio
train$tax_ratio <- (train$tax_property / train$tax_total)
nrow(subset(train,tax_ratio > 1))

#tax_bug
train$tax_bug <- ifelse(train$tax_ratio > 1, 1,0)
table(train$tax_bug)

```
There are only 5 obvious tax errors, so we don't use the tax_bug feature.

### tax of built structure/ total tax
```{r}
train$tax_ratiobuilt <- (train$tax_building / train$tax_total)
```

This may not be a good feature since the correlation is still low.

### Area
### lot_living, lot_bug
lot_living = area_lot / area_live_finished
```{r}
#lot_living
train$lot_living<- (train$area_lot / train$area_live_finished)
summary(train$lot_living)
nrow(subset(train, lot_living < 1))

#log_bug
train$lot_bug <- ifelse(train$lot_living < 1 , 1,0)
table(train$lot_bug)
```

Since lot area should always be bigger than the living area, the lot_living with number smaller than one must be errors. We generate a new feature lot_bug.

### Room number
### bedroom/bathroom ratio
```{r}
train$bbratio<-(train$num_bedroom/(train$num_bathroom+1))
```

## Features to use:
### Original features
num_bathroom
num_bedroom
tax_building
area_total_calc
area_live_finished
tax_property
area_lot
build_year
county
zoning_landuse_county
aircon
heating
flag_tub
flag_spa
flag_pool_spa
flag_pool_tub
flag_fireplace
region_zip
region_county
zoning_landuse

### New features
age of hourse: age
tax ratio: tax_ratio
tax of built structure/ total tax: tax_ratiobuilt
lot area/living area: lot_living
bedroom/bathroom ratio: bbratio
room_bug
lot_bug


```{r}
train.use <- subset(train[,c('logerror','num_bathroom','num_bedroom','tax_building','area_total_calc','area_live_finished','tax_property','area_lot','build_year','county','zoning_landuse_county','aircon','heating','flag_tub','flag_spa','flag_pool_spa','flag_pool_tub','flag_fireplace','age','tax_ratio','tax_ratiobuilt','lot_living','bbratio','region_zip','region_city','region_county','zoning_landuse','room_bug','lot_bug', 'longitude','latitude')])

str(train.use)

```

## Impute missing values
### Check missing values
```{r}
library(Amelia)
missmap(train.use)
missingcol <- colSums(is.na(train.use))
missingcol
```


## drop uni-value column
```{r}
multi_value = sapply(train.use,
                       function(x){
                         return(!length(unique(x[!is.na(x)])) == 1)})
  if(length(multi_value)!=0)
    train.use = train.use[, names(which(multi_value))]
```

## Option one: fill numerical NA with mice
```{r}
library(mice)
# impute numeric variable
num.col = names(which(sapply(train.use, is.numeric)))
train.use.num = train.use[,num.col]
if(sum(is.na(train.use.num))!=0){
  train.use.num.complete = complete(mice(train.use.num, method = "cart", printFlag = F), 1)
  train.use[,num.col] = train.use.num.complete
}
train.use = na.omit(train.use)
train.use[,num.col]=scale(train.use[,num.col])
train.use = train.use[,colSums(is.na(train.use))<nrow(train.use)]
```

## drop uni-value column again
```{r}
multi_value = sapply(train.use,
                       function(x){
                         return(!length(unique(x[!is.na(x)])) == 1)})
  if(length(multi_value)!=0)
    train.use = train.use[, names(which(multi_value))]
```

## Option two: Drop missing values
```{r}
train = train.use[complete.cases(train.use), ]
```

## drop outliers
```{r}
train = train[which(!train$logerror %in% boxplot.stats(train$logerror)$out), ]
```
boxplot.stats()$out use the Tukey's method to identify the outliers ranged above and below the 1.5*IQR.

# Step Three - Training models on the data
# Linear Regression Model
## Standardize
```{r}
train.use$build_year <- as.character(train.use$build_year)
num.col = names(which(sapply(train.use, is.numeric)))
use.num = train.use[,num.col]
train.use[,num.col]=scale(train.use[,num.col])  
```

## Basic linear regression model
```{r}
mod1 <- lm(logerror ~., data = train.use)
plot(mod1)
summary(mod1)
```

## glmnet function
```{r}
library(glmnet)
train.sub <- train.use[which(apply(train.use, 1, 
                                   function(x) length(which(is.na(x))) == 0)), ]  
dim(train.sub)
dep = train.sub$logerror
ind <- model.matrix( ~.-1, train.sub[, -1]) 

```

## Lasso
```{r}
fit_lasso <- glmnet(x=ind, y=dep, alpha = 1)
plot(fit_lasso, xvar = "lambda", label = T)
```

### use cross validation to get optimal value of lambda
```{r}
cvfit1 <- cv.glmnet(ind, dep, alpha = 1)
plot(cvfit1)

sprintf ("lambda.min: %f ",cvfit1$lambda.min)
x1 = coef(cvfit1, s = "lambda.min")
x1
```

### calculate r-square
```{r}
y_pred1 = predict(cvfit1,newx=ind,type="response",s="lambda.min")
sst1 <- sum((train.sub$logerror - mean(train.sub$logerror))^2)
sse1 <- sum((y_pred1 - train.sub$logerror)^2)

r.square1 <- 1 - sse1 / sst1
sprintf ("r_square: %f ",r.square1)
```

## Ridge
```{r}
fit_ridge <- glmnet(x=ind, y=dep, alpha = 0)
plot(fit_ridge, xvar ="lambda", label = T)
```

### use cross validation to get optimal value of lambda
```{r}
cvfit2 <- cv.glmnet(ind, dep, alpha = 0)
plot(cvfit2)

sprintf ("lambda.min: %f ",cvfit2$lambda.min)
x2 = coef(cvfit2, s = "lambda.min")
x2
```

### calculate r-square
```{r}
y_pred2 = predict(cvfit2,newx=ind,type="response",s="lambda.min")
sst2 <- sum((train.sub$logerror - mean(train.sub$logerror))^2)
sse2 <- sum((y_pred2 - train.sub$logerror)^2)

r.square2 <- 1 - sse2 / sst2
sprintf ("r_square: %f ",r.square2)
```

# Tree based Models
## Split the dataset into train and testing
```{r}
set.seed(134)
train.ind <- sample(1:dim(train)[1], dim(train)[1]*0.7)    #randomly select 70% as training data
train.data <- train[train.ind,]
test.data <- train[-train.ind,] 
num.NA1 <- sort(colSums(sapply(test.data, is.na)))   #find how many na in each column and order asc
num.NA1
```

# Desition Tree
## One: Display some trees
### Root only tree, cp=1
```{r}
library(rpart)
formula <- paste("logerror ~ county + tax_land + tax_building + tax_property + area_total_calc + area_live_finished + area_lot + aircon + heating + age + tax_ratio + tax_ratiobuilt + lot_living + zoning_landuse + bbratio + room_bug +  lot_bug + longitude + latitude")

tree0 <- rpart(formula, method = "anova", data = train.data,
               control = rpart.control(cp = 1))
tree0

```

### More complex tree
```{r}
tree1 <- rpart(formula, method = "anova", data = train.data,
               control = rpart.control(cp = 0.0002))
printcp(tree1)
plotcp(tree1)
```
We can see the part in the graph that with cp becoming smaller, tree becoming more complex, though bias decreases, variance increases more, resulting in an increase of total cross validation error (overfitting).

## Two: Pick the tree size that minimizes xerror
```{r}
bestcp1 <- tree1$cptable[which.min(tree1$cptable[,"xerror"]),"CP"]
bestcp1

# Rule of thumb to a simpler model: xerror - 1std, xerror + 1std contains xerror_min
cp.tab <- as.data.frame(tree1$cptable)
cp.tab
with(cp.tab, min(which(xerror - xstd < min(xerror))))
bestcp2 <- cp.tab$CP[with(cp.tab, min(which(xerror - xstd < min(xerror))))]
bestcp2
```
Here, we select cp=0.0009643048 to minimize xerror.

## Three: Prune the tree using the best cp
```{r}
tree.pruned <- prune(tree1, cp = bestcp1) #choose the bestcp and to see what the tree look like now
tree.pruned
```

## Four: use the tree model we train to predict
For character variables, if new levels in test data, there will be errors. So We impute them as NAs.
```{r}
test.data$region_zip[which(!test.data$region_zip %in% train.data$region_zip)] <- NA
test.data$zoning_landuse[which(!test.data$zoning_landuse %in% train.data$zoning_landuse)] <- NA
test.data$county[which(!test.data$county %in% train.data$county)] <- NA
test.data$aircon[which(!test.data$aircon %in% train.data$aircon)] <- NA
test.data$heating[which(!test.data$heating %in% train.data$heating)] <- NA
```

### tree visualization (when tree is simple)
```{r}
library(rpart.plot)
prp(tree.pruned, faclen = 0, cex = 0.8 )   
```

### Prediction - MSE
```{r}
pred.tree <- predict(tree.pruned, test.data)
sum((pred.tree - test.data$logerror)^2) / dim(test.data)[1]
```

# Random Forest
First, we need to convert numeric categorical features into factors, instead of characters, because Random forest library will automatically convert characters into numerics.
```{r}
train$aircon <- as.factor(train$aircon)
train$heating <- as.factor(train$heating)
train$county <- as.factor(train$county)
train$zoning_landuse <- as.factor(train$zoning_landuse)
```

### Train random forest model
```{r}
library(randomForest)
set.seed(123)

rf <- randomForest(as.formula(formula), data = train.data, 
                   importance = TRUE, ntree = 50) 
head(getTree(rf, k = 1, labelVar = TRUE))

#importance of features 
varImpPlot(rf) 
importance(rf, type = 1)  #type = 1 means %IncMSE

#rf performance
plot(rf)  

```
We can see that tax related features, age of the house and location(longitude and latitude) are the most important features. Also notice that the out-of-bag error decreases rapidly first with increasing tree size, and becomes relatively stable after nrounds=30.

### Prediction - MSE
```{r}
pred.rf <- predict(rf, test.data) 
sum((pred.rf- test.data$logerror)^2, na.rm = T) / length(which(!is.na(pred.rf))) 
```

# Xgboost
First, we need to set the label (response variable).
```{r}
library(xgboost)
train.label <- train.data$logerror 
test.label <- test.data$logerror   

formula.boost <- paste("logerror ~ county + tax_land + tax_building + tax_property + area_total_calc + area_live_finished + area_lot + heating + age + tax_ratio + tax_ratiobuilt + lot_living + zoning_landuse + bbratio + room_bug +  lot_bug + longitude + latitude")

feat.cols <- c('county','tax_land','tax_building' ,'tax_property','area_total_calc', 'area_live_finished' ,'area_lot', 'heating' ,'age', 'tax_ratio' ,'tax_ratiobuilt', 'lot_living' ,'zoning_landuse','bbratio' , 'room_bug', 'lot_bug','longitude','latitude')

```

Then,as xgboost only takes matrix features, so we need to convert train.data from data frame to matrix.
```{r}
feature.matrix <- model.matrix(~.-1, data = train.data[, feat.cols])
test.matrix <- model.matrix(~.-1, data = test.data[, feat.cols])
dim(feature.matrix)
dim(train.data)

dim(test.matrix)
dim(test.data)
```

### Self-defined Xgboost
First, we try a self-defined boosting tree.
```{r}
gbt0 <- xgboost(data = feature.matrix, 
               label = train.label,
               max_depth = 8,
               nround = 20,    
               objective = "reg:linear",
               verbose = 2
)

importance <- xgb.importance(feature_names = colnames(feature.matrix),
                             model = gbt0)
head(importance)

#plot gain
xgb.plot.importance(importance, cex =1.2, top_n = 15)
```
In the boosting model, we can see that area, tax, location(longitude and latitude) and age of house freatures make greatest contributions.

#### maximum tree size
```{r}
par <- list( max_depth = 8,
             objective = "reg:linear",
             nthread = 3,
             verbose = 2)
gbt0.cv <- xgb.cv(params = par,
                 data = feature.matrix, label = train.label,
                 nfold = 5, nrounds = 100, verbose = F)

plot(gbt0.cv$evaluation_log$train_rmse_mean, type = 'l')
lines(gbt0.cv$evaluation_log$test_rmse_mean, col = 'red')


nround = which(gbt0.cv$evaluation_log$test_rmse_mean == min(gbt0.cv$evaluation_log$test_rmse_mean))
nround
```
We can see that RMSE for both train and testing dataset decreases greatly first, and become relatively stable afterwards. The optimal nround is 17 to get minimum RMSE on test data.

```{r}
gbt1 <- xgboost(data = feature.matrix, 
               label = train.label,
               nround = nround,
               params = par)
```
Now, gbt1 shows the boosting tree with optimal tree size we just find.

### grid searching for parameters
To get the optimal combination of multiple parameters, we use grid search method.
```{r}
all_param = NULL
all_test_rmse = NULL
all_train_rmse = NULL


for (iter in 1:100) {
  
  param <- list(objective = "reg:linear",
                max_depth = sample(5:12, 1),
                subsample = runif(1, .6, .9),
                colsample_bytree = runif(1, .5, .8),
                eta = runif(1, .01, .3)
                #  gamma = runif(1, 0.0, 0.2),
                #  min_child_weight = sample(1:40, 1),
                #  max_delta_step = sample(1:10, 1)
  )
  cv.nround = 100
  cv.nfold = 5
  seed.number = sample.int(10000, 1)[[1]]
  set.seed(seed.number)
  mdcv <- xgb.cv(data=feature.matrix,
                 label = train.label,
                 params = param, 
                 nfold=cv.nfold,
                 nrounds=cv.nround,
                 #metrics = "mae",
                 early_stopping_rounds = 10, 
                 maximize=FALSE,
                 verbose = F)
  min_train_rmse = min(mdcv$evaluation_log$train_rmse_mean)
  min_test_rmse = min(mdcv$evaluation_log$test_rmse_mean)
  
  all_param <- rbind(all_param, unlist(param)[-1])
  all_train_rmse <- c(all_train_rmse, min_train_rmse)
  all_test_rmse <- c(all_test_rmse, min_test_rmse)
}

all_param <- as.data.frame(all_param)
best_param <- all_param[which(all_test_rmse == min(all_test_rmse)), ]
best_param
gbt <- xgboost(data =  feature.matrix, 
               label = train.label, 
               params = best_param,
               nrounds=100,
               early_stopping_rounds = 10,
               maximize = FALSE)

# prediction
prediction <- predict(gbt, test.matrix)
mean((prediction - test.data$logerror)^2)
```

# Step Four - Evaluating model performance
## prediction summary - MSE
```{r}
#Decistin Tree: 
MSE.DT <- mean((pred.tree - test.data$logerror)^2)
sprintf("Decision Tree MSE: %f", MSE.DT)
#Random Forest:
MSE.RF <- sum((pred.rf- test.data$logerror)^2, na.rm = T) / length(which(!is.na(pred.rf))) 
sprintf("Random Forest MSE: %f", MSE.RF)
#Boosting:
MSE.Boosting <- mean((prediction - test.data$logerror)^2)
sprintf("Boosting MSE: %f", MSE.Boosting)
```






















